{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://cognitiveclass.ai\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"400\"> </a>\n",
    "\n",
    "<h1 align=center><font size = 5>Pre-Trained Models</font></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will learn how to leverage pre-trained models to build image classifiers instead of building a model from scratch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "\n",
    "<font size = 3> \n",
    "    \n",
    "1. <a href=\"#item31\">Import Libraries and Packages</a>\n",
    "2. <a href=\"#item32\">Download Data</a>  \n",
    "3. <a href=\"#item33\">Define Global Constants</a>  \n",
    "4. <a href=\"#item34\">Construct ImageDataGenerator Instances</a>  \n",
    "5. <a href=\"#item35\">Compile and Fit Model</a>\n",
    "\n",
    "</font>\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item31'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Packages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start the lab by importing the libraries that we will be using in this lab. First we will need the library that helps us to import the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import skillsnetwork "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will import the ImageDataGenerator module since we will be leveraging it to train our model in batches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will be using the Keras library to build an image classifier, so let's download the Keras library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will be leveraging the ResNet50 model to build our classifier, so let's download it as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.applications import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item32'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you are going to download the data from IBM object storage using **skillsnetwork.prepare** command. skillsnetwork.prepare is a command that's used to download a zip file, unzip it and store it in a specified directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ebccaebba31475691babb537a21885b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading concrete_data_week3.zip:   0%|          | 0/97863179 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c756dd38dc0471299bf44284ef15ced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30036 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to '.'\n"
     ]
    }
   ],
   "source": [
    "## get the data\n",
    "await skillsnetwork.prepare(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/concrete_data_week3.zip\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should see the folder *concrete_data_week3* appear in the left pane. If you open this folder by double-clicking on it, you will find that it contains two folders: *train* and *valid*. And if you explore these folders, you will find that each contains two subfolders: *positive* and *negative*. These are the same folders that we saw in the labs in the previous modules of this course, where *negative* is the negative class and it represents the concrete images with no cracks and *positive* is the positive class and it represents the concrete images with cracks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note**: There are thousands and thousands of images in each folder, so please don't attempt to double click on the *negative* and *positive* folders. This may consume all of your memory and you may end up with a **50** error. So please **DO NOT DO IT**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item33'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Global Constants\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will define constants that we will be using throughout the rest of the lab. \n",
    "\n",
    "1. We are obviously dealing with two classes, so *num_classes* is 2. \n",
    "2. The ResNet50 model was built and trained using images of size (224 x 224). Therefore, we will have to resize our images from (227 x 227) to (224 x 224).\n",
    "3. We will training and validating the model using batches of 100 images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "\n",
    "image_resize = 224\n",
    "\n",
    "batch_size_training = 100\n",
    "batch_size_validation = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item34'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct ImageDataGenerator Instances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to instantiate an ImageDataGenerator instance, we will set the **preprocessing_function** argument to *preprocess_input* which we imported from **keras.applications.resnet50** in order to preprocess our images the same way the images used to train ResNet50 model were processed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_generator = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use the *flow_from_directory* method to get the training images as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/train',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_training,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: in this lab, we will be using the full data-set of 30,000 images for training and validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Turn**: Use the *flow_from_directory* method to get the validation images and assign the result to **validation_generator**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "## Type your answer here\n",
    "\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/valid',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_training,\n",
    "    class_mode='categorical')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click __here__ for the solution.\n",
    "<!-- The correct answer is:\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/valid',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_validation,\n",
    "    class_mode='categorical')\n",
    "-->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item35'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build, Compile and Fit Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will start building our model. We will use the Sequential model class from Keras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:68: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will add the ResNet50 pre-trained model to out model. However, note that we don't want to include the top layer or the output layer of the pre-trained model. We actually want to define our own output layer and train it so that it is optimized for our image dataset. In order to leave out the output layer of the pre-trained model, we will use the argument *include_top* and set it to **False**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:508: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3837: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:168: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:175: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1801: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3661: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-24 09:09:48.781221: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "2024-08-24 09:09:48.785837: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2394315000 Hz\n",
      "2024-08-24 09:09:48.786429: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5633f664dd10 executing computations on platform Host. Devices:\n",
      "2024-08-24 09:09:48.786482: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2024-08-24 09:09:48.818635: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3665: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94658560/94653016 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "model.add(ResNet50(\n",
    "    include_top=False,\n",
    "    pooling='avg',\n",
    "    weights='imagenet',\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will define our output layer as a **Dense** layer, that consists of two nodes and uses the **Softmax** function as the activation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the model's layers using the *layers* attribute of our model object. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.training.Model at 0x7fa36e83b250>,\n",
       " <keras.layers.core.Dense at 0x7fa36c415b50>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that our model is composed of two sets of layers. The first set is the layers pertaining to ResNet50 and the second set is a single layer, which is our Dense layer that we defined above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the ResNet50 layers by running the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.topology.InputLayer at 0x7fa3e23b7ed0>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7fa3e2393290>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa3e23b75d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa4344c1310>,\n",
       " <keras.layers.core.Activation at 0x7fa3f0677d90>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7fa3e00b6790>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa3e2383e90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa3e3f7ec90>,\n",
       " <keras.layers.core.Activation at 0x7fa3e3f7e8d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa3e3f3ee10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa3e3e91750>,\n",
       " <keras.layers.core.Activation at 0x7fa3e3e91310>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa3e3e2d410>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa3e3d26dd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa3e3d8edd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa3e3c9d190>,\n",
       " <keras.layers.merge.Add at 0x7fa3e3c6abd0>,\n",
       " <keras.layers.core.Activation at 0x7fa3e3c25d50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa3e3b441d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa3e3b98b50>,\n",
       " <keras.layers.core.Activation at 0x7fa3e3b32490>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa3e3ad6d90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa3e242c9d0>,\n",
       " <keras.layers.core.Activation at 0x7fa3e22d8f50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa3e228e650>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa3e22ab2d0>,\n",
       " <keras.layers.merge.Add at 0x7fa3e224e910>,\n",
       " <keras.layers.core.Activation at 0x7fa3e2255410>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa3e2255550>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa3e393cb10>,\n",
       " <keras.layers.core.Activation at 0x7fa3e393cfd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa3e384f990>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa3bc78db10>,\n",
       " <keras.layers.core.Activation at 0x7fa3bc7c0190>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa3bc75e950>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa3bc67d7d0>,\n",
       " <keras.layers.merge.Add at 0x7fa3bc6c29d0>,\n",
       " <keras.layers.core.Activation at 0x7fa3bc6613d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa3bc678a10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa3bc5daf10>,\n",
       " <keras.layers.core.Activation at 0x7fa3bc5da6d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa3bc574e90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa3bc4def50>,\n",
       " <keras.layers.core.Activation at 0x7fa3bc4deed0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa3bc40a550>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa3bc326950>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa3bc3a8750>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa3bc285f50>,\n",
       " <keras.layers.merge.Add at 0x7fa3bc2abe10>,\n",
       " <keras.layers.core.Activation at 0x7fa3bc221f10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa3bc166ed0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa3bc199590>,\n",
       " <keras.layers.core.Activation at 0x7fa3bc199f10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa3bc0bdfd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa3bc045f50>,\n",
       " <keras.layers.core.Activation at 0x7fa3bc09c150>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa3ac7f5810>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa3ac716290>,\n",
       " <keras.layers.merge.Add at 0x7fa3ac774090>,\n",
       " <keras.layers.core.Activation at 0x7fa3ac68e150>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa3ac61e750>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa3ac606450>,\n",
       " <keras.layers.core.Activation at 0x7fa3ac606ed0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa3ac5aab10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa3ac508650>,\n",
       " <keras.layers.core.Activation at 0x7fa3ac508750>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa3ac4a1d90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa3ac40add0>,\n",
       " <keras.layers.merge.Add at 0x7fa3ac40afd0>,\n",
       " <keras.layers.core.Activation at 0x7fa3ac339090>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa3ac2cb6d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa3ac31bcd0>,\n",
       " <keras.layers.core.Activation at 0x7fa3ac332e50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa3ac255ad0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa3ac233610>,\n",
       " <keras.layers.core.Activation at 0x7fa3ac233710>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa3ac14d590>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa3ac136fd0>,\n",
       " <keras.layers.merge.Add at 0x7fa3ac0c9a90>,\n",
       " <keras.layers.core.Activation at 0x7fa3ac068710>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa38c7b5650>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa38c788c90>,\n",
       " <keras.layers.core.Activation at 0x7fa38c7a2dd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa38c6caa90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa38c69e450>,\n",
       " <keras.layers.core.Activation at 0x7fa38c69efd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa38c5ba550>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa38c4d4410>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa38c5a0f90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa38c4f01d0>,\n",
       " <keras.layers.merge.Add at 0x7fa38c416290>,\n",
       " <keras.layers.core.Activation at 0x7fa38c3d5a10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa38c389450>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa38c351f50>,\n",
       " <keras.layers.core.Activation at 0x7fa38c351490>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa38c284290>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa38c264ed0>,\n",
       " <keras.layers.core.Activation at 0x7fa38c264910>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa38c180090>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa38c0fa190>,\n",
       " <keras.layers.merge.Add at 0x7fa38c0fa350>,\n",
       " <keras.layers.core.Activation at 0x7fa38c0962d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa38c096a90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa36ffd6f90>,\n",
       " <keras.layers.core.Activation at 0x7fa36ffd6d90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa36ff77d90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa36fedddd0>,\n",
       " <keras.layers.core.Activation at 0x7fa36feddfd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa36fe08410>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa36fdeda50>,\n",
       " <keras.layers.merge.Add at 0x7fa36fdedad0>,\n",
       " <keras.layers.core.Activation at 0x7fa36fd0bc10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa36fd3f210>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa36fc832d0>,\n",
       " <keras.layers.core.Activation at 0x7fa36fc833d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa36fc3df90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa36fb9e690>,\n",
       " <keras.layers.core.Activation at 0x7fa36fb9e790>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa36fb34dd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa36faa1a10>,\n",
       " <keras.layers.merge.Add at 0x7fa36faa1e10>,\n",
       " <keras.layers.core.Activation at 0x7fa36f9ce210>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa36f915250>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa36f9b1390>,\n",
       " <keras.layers.core.Activation at 0x7fa36f948e90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa36f8e3950>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa36f848fd0>,\n",
       " <keras.layers.core.Activation at 0x7fa36f7e7950>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa36f7fdf50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa36f719390>,\n",
       " <keras.layers.merge.Add at 0x7fa36f75ef90>,\n",
       " <keras.layers.core.Activation at 0x7fa36f6f8ed0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa36f6f8e90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa36f5d0f50>,\n",
       " <keras.layers.core.Activation at 0x7fa36f675bd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa36f5b7a90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa36f50b490>,\n",
       " <keras.layers.core.Activation at 0x7fa36f50b750>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa36f4a8610>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa36f422f50>,\n",
       " <keras.layers.merge.Add at 0x7fa36f43d250>,\n",
       " <keras.layers.core.Activation at 0x7fa36f3bfe10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa36f2cc550>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa36f325c90>,\n",
       " <keras.layers.core.Activation at 0x7fa36f325150>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa36f255150>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa36f238b10>,\n",
       " <keras.layers.core.Activation at 0x7fa36f3bf050>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa36f155950>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa36f00de50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa36f17a410>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa36f0231d0>,\n",
       " <keras.layers.merge.Add at 0x7fa36ef69550>,\n",
       " <keras.layers.core.Activation at 0x7fa36ef7ef10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa36eea0690>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa36eefd510>,\n",
       " <keras.layers.core.Activation at 0x7fa36eefdfd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa36ee1ba90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa36ed94250>,\n",
       " <keras.layers.core.Activation at 0x7fa36ed60550>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa36ed30d50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa36ec99e50>,\n",
       " <keras.layers.merge.Add at 0x7fa36ec99f10>,\n",
       " <keras.layers.core.Activation at 0x7fa36ec2ff50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa36ec2ff90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa36eba9290>,\n",
       " <keras.layers.core.Activation at 0x7fa36eba9090>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa36eaeea50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa36ea41410>,\n",
       " <keras.layers.core.Activation at 0x7fa36ea41f90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa36e9db790>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa36e943e50>,\n",
       " <keras.layers.merge.Add at 0x7fa36e943d10>,\n",
       " <keras.layers.core.Activation at 0x7fa36e8f5590>,\n",
       " <keras.layers.pooling.AveragePooling2D at 0x7fa36ec2f4d0>,\n",
       " <keras.layers.pooling.GlobalAveragePooling2D at 0x7fa3e3ffd410>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the ResNet50 model has already been trained, then we want to tell our model not to bother with training the ResNet part, but to train only our dense output layer. To do that, we run the following.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now using the *summary* attribute of the model, we can see how many parameters we will need to optimize in order to train the output layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Model)             (None, 2048)              23587712  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 4098      \n",
      "=================================================================\n",
      "Total params: 23,591,810\n",
      "Trainable params: 4,098\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we compile our model using the **adam** optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/optimizers.py:757: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we are able to start the training process, with an ImageDataGenerator, we will need to define how many steps compose an epoch. Typically, that is the number of images divided by the batch size. Therefore, we define our steps per epoch as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "steps_per_epoch_training = len(train_generator)\n",
    "steps_per_epoch_validation = len(validation_generator)\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to start training our model. Unlike a conventional deep learning training were data is not streamed from a directory, with an ImageDataGenerator where data is augmented in batches, we use the **fit_generator** method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/2\n",
      "  5/101 [>.............................] - ETA: 1:42:05 - loss: 0.4761 - acc: 0.7800"
     ]
    }
   ],
   "source": [
    "fit_history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch_training,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=steps_per_epoch_validation,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, you are ready to start using it to classify images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since training can take a long time when building deep learning models, it is always a good idea to save your model once the training is complete if you believe you will be using the model again later. You will be using this model in the next module, so go ahead and save your model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('classifier_resnet_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should see the model file *classifier_resnet_model.h5* apprear in the left directory pane.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thank you for completing this lab!\n",
    "\n",
    "This notebook was created by Alex Aklson. I hope you found this lab interesting and educational.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of a course on **Coursera** called *AI Capstone Project with Deep Learning*. If you accessed this notebook outside the course, you can take this course online by clicking [here](https://cocl.us/DL0321EN_Coursera_Week3_LAB1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Change Log\n",
    "\n",
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "| 2020-09-18  | 2.0  | Shubham  |  Migrated Lab to Markdown and added to course repo in GitLab |\n",
    "| 2023-01-03  | 3.0  | Artem |  Updated the file import section|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Copyright &copy; 2020 [IBM Developer Skills Network](https://cognitiveclass.ai/?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "prev_pub_hash": "cf2970a1d2c549fe86023eaa076d0ce4936c4275baf2cccfdad8fe6ce3a8a6c2"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
